{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xOK77kzLaBip"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13208\\3573617612.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#connect to google drive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/gdrive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "#connect to google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hIAmuMKwZwvO"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13208\\3433338953.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mGRU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSeparableConv1D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSimpleRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSeparableConv1D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPermute\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDepthwiseConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGlobalMaxPooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#init libs\n",
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "from tensorflow.keras import Input, Model, Sequential, regularizers\n",
    "from tensorflow.keras.layers import concatenate,TimeDistributed, Concatenate,  GRU, Activation, Dropout, Flatten,SeparableConv1D, Dense, SimpleRNN, LSTM, Conv1D,SeparableConv1D, Reshape,MaxPooling1D, Conv2D, Reshape,MaxPooling2D, Permute,Bidirectional, BatchNormalization, DepthwiseConv2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "from datetime import datetime\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msWKZXwp8ypp"
   },
   "outputs": [],
   "source": [
    "#random generator for regression\n",
    "\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# plot 1\n",
    "#for clustering and multiclass classification\n",
    "X_clust, Y_clust = make_blobs(n_samples=10000,n_features=10, centers=7)\n",
    "#for binary classification\n",
    "X_class, Y_class = make_classification(n_samples=10000, n_features=10, n_informative=5, n_redundant=4, random_state=1)\n",
    "#for regression\n",
    "X_regr, Y_regr = make_regression(n_samples=10000, n_features=10, n_informative=5, random_state=1)\n",
    "\n",
    "#show examples\n",
    "X, y = make_blobs(n_features=2, centers=2)\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.savefig('centers_1.png')\n",
    "plt.title('centers = 2')\n",
    "\n",
    "# plot 2    \n",
    "X, y = make_blobs(n_features=2, centers=4)\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title('centers = 4')\n",
    "\n",
    "# plot 3\n",
    "X, y = make_blobs(n_samples=100,n_features=2, centers=7)\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title('centers = 7')\n",
    "print(len(X))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vla6FSWtGYVg"
   },
   "outputs": [],
   "source": [
    "#check data\n",
    "print(X_class[0])\n",
    "print(Y_class[0])\n",
    "print(\"---------\")\n",
    "print(X_clust[0])\n",
    "print(Y_clust[0])\n",
    "print(\"---------\")\n",
    "print(X_regr[0])\n",
    "print(Y_regr[0])\n",
    "print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibobX-kLAz8S"
   },
   "outputs": [],
   "source": [
    "#stanartize data\n",
    "mean_class = X_class.mean(axis=0)\n",
    "std_class = X_class.std(axis=0)\n",
    "X_class -= mean_class\n",
    "X_class /= std_class\n",
    "\n",
    "\n",
    "#stanartize data\n",
    "mean_regr = X_regr.mean(axis=0)\n",
    "std_regr = X_regr.std(axis=0)\n",
    "X_regr -= mean_regr\n",
    "X_regr /= std_regr\n",
    "\n",
    "#stanartize data\n",
    "mean_clust = X_clust.mean(axis=0)\n",
    "std_clust = X_clust.std(axis=0)\n",
    "X_clust -= mean_clust\n",
    "X_clust /= std_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96Mx6LpcSPqL"
   },
   "outputs": [],
   "source": [
    "#split dataset to train and test datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_regr, X_test_regr, y_train_regr, y_test_regr = train_test_split(X_regr, Y_regr, test_size=0.33, random_state=1)\n",
    "\n",
    "X_train_clust, X_test_clust, y_train_clust, y_test_clust = train_test_split(X_clust, Y_clust, test_size=0.33, random_state=1)\n",
    "\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class, Y_class, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aidqClbc_x1x"
   },
   "outputs": [],
   "source": [
    "#experiment N1 with linear regression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#see full docs here https://scikit-learn.org/stable/modules/linear_model.html\n",
    "model = LinearRegression()\n",
    "# fit the model\n",
    "model.fit(X_train_regr, y_train_regr)\n",
    "# get importance\n",
    "importance = model.coef_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "print(\"value big - important\")\n",
    "\n",
    "#score - Ordinary Least Squares\n",
    "score_test=model.score(X_test_regr,y_test_regr)\n",
    "score_train=model.score(X_train_regr, y_train_regr)\n",
    "\n",
    "yhat = model.predict(X_test_regr)\n",
    "\n",
    "for i in range(10):\n",
    "\tprint(\"real is \"+str(y_test_regr[i])+\";  predicted is \"+str(yhat[i]))\n",
    "\n",
    "print(\"--------\")\n",
    "print(\"train \"+str(score_train))\n",
    "print(\"test \"+str(score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QFhFT47WkS7"
   },
   "outputs": [],
   "source": [
    "#create custom metric (for binary classification)\n",
    "def custom_accuracy_metric(yhat, yreal):\n",
    "  mcounter=0\n",
    "  for i in range(len(yhat)):\n",
    "    c=0\n",
    "    if yhat[i]<0.5:\n",
    "      c=0\n",
    "    else:\n",
    "      c=1\n",
    "    if c==yreal[i]:\n",
    "      mcounter=mcounter+1\n",
    "  return mcounter/len(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxAZR2-SArnT"
   },
   "outputs": [],
   "source": [
    "#experiment N2 with logistic regression (for binary classification)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#full description is here https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "model = LogisticRegression()\n",
    "# fit the model\n",
    "model.fit(X_train_class, y_train_class)\n",
    "# get importance\n",
    "importance = model.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "print(\"negative values are important for 0, positive for 1 \")\n",
    "\n",
    "yhat = model.predict(X_test_class)\n",
    "acc=custom_accuracy_metric(yhat,y_test_class)\n",
    "\n",
    "#score - accuracy\n",
    "score_test=model.score(X_test_class,y_test_class)\n",
    "score_train=model.score(X_train_class, y_train_class)\n",
    "print(\"--------\")\n",
    "print(\"train \"+str(score_train))\n",
    "print(\"test \"+str(score_test))\n",
    "print(\"custom test \"+str(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KESpoU3IBpBD"
   },
   "outputs": [],
   "source": [
    "#experiment N3 with Decision tree refression \n",
    "\n",
    "# decision tree for feature importance on a regression problem\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "# define the model\n",
    "model = DecisionTreeRegressor()\n",
    "# fit the model\n",
    "model.fit(X_train_regr, y_train_regr)\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "\n",
    "score_test=model.score(X_test_regr,y_test_regr)\n",
    "score_train=model.score(X_train_regr, y_train_regr)\n",
    "\n",
    "yhat = model.predict(X_test_regr)\n",
    "\n",
    "for i in range(10):\n",
    "\tprint(\"real is \"+str(y_test_regr[i])+\";  predicted is \"+str(yhat[i]))\n",
    "\n",
    "print(\"--------\")\n",
    "print(\"train \"+str(score_train))\n",
    "print(\"test \"+str(score_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pzh1np-JCISX"
   },
   "outputs": [],
   "source": [
    "#experiment N4 with Decision tree Classifier (for binary classification)\n",
    "\n",
    "# decision tree for feature importance on a classification problem\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define the model\n",
    "model = DecisionTreeClassifier()\n",
    "# fit the model\n",
    "model.fit(X_train_class, y_train_class)\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "yhat = model.predict(X_test_class)\n",
    "acc=custom_accuracy_metric(yhat,y_test_class)\n",
    "\n",
    "#score - accuracy\n",
    "score_test=model.score(X_test_class,y_test_class)\n",
    "score_train=model.score(X_train_class, y_train_class)\n",
    "\n",
    "for i in range(10):\n",
    "\tprint(\"real is \"+str(y_test_class[i])+\";  predicted is \"+str(yhat[i]))\n",
    "\n",
    "\n",
    "print(\"--------\")\n",
    "print(\"train \"+str(score_train))\n",
    "print(\"test \"+str(score_test))\n",
    "print(\"custom test \"+str(acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTSeTnOHCzKL"
   },
   "outputs": [],
   "source": [
    "#experiment N5 with Random Forest Regressor \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "# define the model\n",
    "model = RandomForestRegressor()\n",
    "# fit the model\n",
    "model.fit(X_train_regr, y_train_regr)\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "\n",
    "score_test=model.score(X_test_regr,y_test_regr)\n",
    "score_train=model.score(X_train_regr, y_train_regr)\n",
    "\n",
    "yhat = model.predict(X_test_regr)\n",
    "\n",
    "for i in range(10):\n",
    "\tprint(\"real is \"+str(y_test_regr[i])+\";  predicted is \"+str(yhat[i]))\n",
    "\n",
    "print(\"--------\")\n",
    "print(\"train \"+str(score_train))\n",
    "print(\"test \"+str(score_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4a5F3ayDD8B"
   },
   "outputs": [],
   "source": [
    "#experiment N6 with Random Forest Classifier \n",
    "# random forest for feature importance on a classification problem\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "# fit the model\n",
    "model.fit(X_train_class, y_train_class)\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "\n",
    "yhat = model.predict(X_test_class)\n",
    "acc=custom_accuracy_metric(yhat,y_test_class)\n",
    "\n",
    "#score - accuracy\n",
    "score_test=model.score(X_test_class,y_test_class)\n",
    "score_train=model.score(X_train_class, y_train_class)\n",
    "\n",
    "for i in range(10):\n",
    "\tprint(\"real is \"+str(y_test_class[i])+\";  predicted is \"+str(yhat[i]))\n",
    "\n",
    "\n",
    "print(\"--------\")\n",
    "print(\"train \"+str(score_train))\n",
    "print(\"test \"+str(score_test))\n",
    "print(\"custom test \"+str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcFwOAk2DVLn"
   },
   "outputs": [],
   "source": [
    "#experiment N7 with xgboost for regression task\n",
    "\n",
    "# xgboost for feature importance on a regression problem\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from matplotlib import pyplot\n",
    "model = XGBRegressor()\n",
    "# fit the model\n",
    "model.fit(X_train_regr, y_train_regr)\n",
    "# get importance\n",
    "\n",
    "importance = model.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "score_test=model.score(X_test_regr,y_test_regr)\n",
    "score_train=model.score(X_train_regr, y_train_regr)\n",
    "\n",
    "yhat = model.predict(X_test_regr)\n",
    "\n",
    "for i in range(10):\n",
    "\tprint(\"real is \"+str(y_test_regr[i])+\";  predicted is \"+str(yhat[i]))\n",
    "\n",
    "print(\"--------\")\n",
    "print(\"train \"+str(score_train))\n",
    "print(\"test \"+str(score_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJXj79zmDmAc"
   },
   "outputs": [],
   "source": [
    "#experiment N8 with xgboost for classification task\n",
    "from sklearn.datasets import make_classification\n",
    "from xgboost import XGBClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "model = XGBClassifier()\n",
    "# fit the model\n",
    "model.fit(X_train_class, y_train_class)\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "\n",
    "yhat = model.predict(X_test_class)\n",
    "acc=custom_accuracy_metric(yhat,y_test_class)\n",
    "\n",
    "#score - accuracy\n",
    "score_test=model.score(X_test_class,y_test_class)\n",
    "score_train=model.score(X_train_class, y_train_class)\n",
    "\n",
    "for i in range(10):\n",
    "\tprint(\"real is \"+str(y_test_class[i])+\";  predicted is \"+str(yhat[i]))\n",
    "\n",
    "\n",
    "print(\"--------\")\n",
    "print(\"train \"+str(score_train))\n",
    "print(\"test \"+str(score_test))\n",
    "print(\"custom test \"+str(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8X0KEWNEGqC"
   },
   "outputs": [],
   "source": [
    "#experiment N9 with KNeighborsRegressor for regression task\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "model = KNeighborsRegressor()\n",
    "# fit the model\n",
    "model.fit(X_train_regr, y_train_regr)\n",
    "# permutation feature importance with knn for regression\n",
    "results = permutation_importance(model, X_train_regr, y_train_regr, scoring='neg_mean_squared_error')\n",
    "# get importance\n",
    "importance = results.importances_mean\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "\n",
    "score_test=model.score(X_test_regr,y_test_regr)\n",
    "score_train=model.score(X_train_regr, y_train_regr)\n",
    "\n",
    "yhat = model.predict(X_test_regr)\n",
    "\n",
    "for i in range(10):\n",
    "\tprint(\"real is \"+str(y_test_regr[i])+\";  predicted is \"+str(yhat[i]))\n",
    "\n",
    "print(\"--------\")\n",
    "print(\"train \"+str(score_train))\n",
    "print(\"test \"+str(score_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBOgxpxgEsd0"
   },
   "outputs": [],
   "source": [
    "#experiment N10 with KNeighborsClassifier for classification task\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from matplotlib import pyplot\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "# fit the model\n",
    "model.fit(X_train_class, y_train_class)\n",
    "# perform permutation importance\n",
    "results = permutation_importance(model, X_train_class, y_train_class, scoring='accuracy')\n",
    "# get importance\n",
    "importance = results.importances_mean\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "\n",
    "yhat = model.predict(X_test_class)\n",
    "acc=custom_accuracy_metric(yhat,y_test_class)\n",
    "\n",
    "#score - accuracy\n",
    "score_test=model.score(X_test_class,y_test_class)\n",
    "score_train=model.score(X_train_class, y_train_class)\n",
    "\n",
    "for i in range(10):\n",
    "\tprint(\"real is \"+str(y_test_class[i])+\";  predicted is \"+str(yhat[i]))\n",
    "\n",
    "\n",
    "print(\"--------\")\n",
    "print(\"train \"+str(score_train))\n",
    "print(\"test \"+str(score_test))\n",
    "print(\"custom test \"+str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khTtGuSeHx2p"
   },
   "outputs": [],
   "source": [
    "#experiment N11. make ensemble\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#make ML model 1\n",
    "est_AB = AdaBoostClassifier()\n",
    "score_AB=est_AB.fit(X_train_class, y_train_class).score(X_test_class,y_test_class)\n",
    "\n",
    "\n",
    "results = permutation_importance(est_AB, X_train_class, y_train_class, scoring='accuracy')\n",
    "# get importance\n",
    "importance = results.importances_mean\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "\n",
    "yhat = est_AB.predict(X_test_class)\n",
    "acc=custom_accuracy_metric(yhat,y_test_class)\n",
    "\n",
    "#score - accuracy\n",
    "score_test=est_AB.score(X_test_class,y_test_class)\n",
    "score_train=est_AB.score(X_train_class, y_train_class)\n",
    "\n",
    "for i in range(10):\n",
    "\tprint(\"real is \"+str(y_test_class[i])+\";  predicted is \"+str(yhat[i]))\n",
    "\n",
    "\n",
    "print(\"--------\")\n",
    "print(\"train \"+str(score_train))\n",
    "print(\"test \"+str(score_test))\n",
    "print(\"custom test \"+str(acc))\n",
    "\n",
    "\n",
    "#make ML model 2\n",
    "est_RF = RandomForestClassifier()\n",
    "score_RF=est_RF.fit(X_train_class, y_train_class).score(X_test_class,y_test_class)\n",
    "\n",
    "\n",
    "results = permutation_importance(est_RF, X_train_class, y_train_class, scoring='accuracy')\n",
    "# get importance\n",
    "importance = results.importances_mean\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "yhat = est_RF.predict(X_test_class)\n",
    "acc=custom_accuracy_metric(yhat,y_test_class)\n",
    "\n",
    "#score - accuracy\n",
    "score_test=est_RF.score(X_test_class,y_test_class)\n",
    "score_train=est_RF.score(X_train_class, y_train_class)\n",
    "\n",
    "for i in range(10):\n",
    "\tprint(\"real is \"+str(y_test_class[i])+\";  predicted is \"+str(yhat[i]))\n",
    "\n",
    "\n",
    "print(\"--------\")\n",
    "print(\"train \"+str(score_train))\n",
    "print(\"test \"+str(score_test))\n",
    "print(\"custom test \"+str(acc))\n",
    "\n",
    "\n",
    "\n",
    "#make ensemble based on est_AB and est_RF models\n",
    "\n",
    "est_Ensemble = VotingClassifier(estimators=[('AB', est_AB), ('RF', est_RF)],\n",
    "                        voting='soft',\n",
    "                        weights=[1, 1])\n",
    "\n",
    "score_Ensemble=est_Ensemble.fit(X_train_class, y_train_class).score(X_test_class,y_test_class)\n",
    "\n",
    "yhat = est_Ensemble.predict(X_test_class)\n",
    "acc=custom_accuracy_metric(yhat,y_test_class)\n",
    "\n",
    "score_test=est_Ensemble.score(X_train_class, y_train_class)\n",
    "score_train=est_Ensemble.score(X_train_class, y_train_class)\n",
    "\n",
    "print(\"--------\")\n",
    "print(score_Ensemble)\n",
    "print(\"train \"+str(score_train))\n",
    "print(\"test \"+str(score_test))\n",
    "print(\"custom test \"+str(acc))\n",
    "\n",
    "results = permutation_importance(est_Ensemble, X_train_class, y_train_class, scoring='accuracy')\n",
    "# get importance\n",
    "importance = results.importances_mean\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4gR8-iLSvYO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caSVtOXySu2D"
   },
   "outputs": [],
   "source": [
    "##experiment N12. KMeans clustering \n",
    "#prepare data\n",
    "X_clust, Y_clust = make_blobs(n_samples=10000,n_features=2, centers=7)\n",
    "\n",
    "#stanartize data\n",
    "mean_clust = X_clust.mean(axis=0)\n",
    "std_clust = X_clust.std(axis=0)\n",
    "X_clust -= mean_clust\n",
    "X_clust /= std_clust\n",
    "\n",
    "X_train_clust, X_test_clust, y_train_clust, y_test_clust = train_test_split(X_clust, Y_clust, test_size=0.33, random_state=1)\n",
    "\n",
    "#unique classes\n",
    "classes = set(y_train_clust)\n",
    "print(classes)\n",
    "#print len of datasets\n",
    "print(len(X_test_clust))\n",
    "print(len(X_train_clust))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHKgUwuSSdiB"
   },
   "outputs": [],
   "source": [
    "##experiment N12. KMeans clustering \n",
    "#visualisation of data\n",
    "plt.figure()\n",
    "plt.scatter(X_train_clust[:, 0], X_train_clust[:, 1], c=\"green\")\n",
    "plt.title('clustering no_color source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOiEEUVYSepN"
   },
   "outputs": [],
   "source": [
    "##experiment N12. KMeans clustering \n",
    "#visualisation of data with colors\n",
    "plt.figure()\n",
    "plt.scatter(X_train_clust[:, 0], X_train_clust[:, 1], c=y_train_clust)\n",
    "plt.title('clustering colored source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eSLtoRmRYit"
   },
   "outputs": [],
   "source": [
    "##experiment N12. KMeans clustering \n",
    "#train KMeans clusterisation\n",
    "from sklearn.cluster import KMeans\n",
    "#after data analyzing 4 classes were selected\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(X_train_clust)\n",
    "print(kmeans.labels_)\n",
    "print(len(kmeans.labels_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glUCfzMtSGZA"
   },
   "outputs": [],
   "source": [
    "##experiment N12. KMeans clustering \n",
    "#analyze clustered data\n",
    "plt.figure()\n",
    "plt.scatter(X_train_clust[:, 0], X_train_clust[:, 1], c=kmeans.labels_)\n",
    "plt.title('train_data analyzed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLvkI1d_VH3P"
   },
   "outputs": [],
   "source": [
    "##experiment N12. KMeans clustering \n",
    "#check for test data. analyze clustered data\n",
    "\n",
    "test_labels=kmeans.predict(X_test_clust)\n",
    "plt.figure()\n",
    "plt.scatter(X_test_clust[:, 0], X_test_clust[:, 1], c=test_labels)\n",
    "plt.title('test_data analyzed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIy7Uf0gSRgS"
   },
   "outputs": [],
   "source": [
    "X_train_clust, X_test_clust, y_train_clust, y_test_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_WFIMfvCxFZ"
   },
   "outputs": [],
   "source": [
    "##experiment N13. save and load model. way1\n",
    "import pickle\n",
    "filename = 'MY_finalized_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "# some time later...\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "yhat = model.predict(X_test_class)\n",
    "acc=custom_accuracy_metric(yhat,y_test_class)\n",
    "\n",
    "#score - accuracy\n",
    "score_test=model.score(X_test_class,y_test_class)\n",
    "score_train=model.score(X_train_class, y_train_class)\n",
    "\n",
    "for i in range(10):\n",
    "\tprint(\"real is \"+str(y_test_class[i])+\";  predicted is \"+str(yhat[i]))\n",
    "\n",
    "\n",
    "print(\"--------\")\n",
    "print(\"train \"+str(score_train))\n",
    "print(\"test \"+str(score_test))\n",
    "print(\"custom test \"+str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VeZx7HldrlQ"
   },
   "outputs": [],
   "source": [
    "##experiment N13. save and load model. way2\n",
    "import joblib\n",
    "# save the model to disk\n",
    "filename = 'MY_finalized_model.sav'\n",
    "joblib.dump(model, filename)\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = joblib.load(filename)\n",
    "\n",
    "yhat = model.predict(X_test_class)\n",
    "acc=custom_accuracy_metric(yhat,y_test_class)\n",
    "\n",
    "#score - accuracy\n",
    "score_test=model.score(X_test_class,y_test_class)\n",
    "score_train=model.score(X_train_class, y_train_class)\n",
    "\n",
    "for i in range(10):\n",
    "\tprint(\"real is \"+str(y_test_class[i])+\";  predicted is \"+str(yhat[i]))\n",
    "\n",
    "\n",
    "print(\"--------\")\n",
    "print(\"train \"+str(score_train))\n",
    "print(\"test \"+str(score_test))\n",
    "print(\"custom test \"+str(acc))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SKLEARN_ML_models.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
